:walkthrough: Cloud-Native Integration with EIPs
:codeready-url: http://codeready-che.{openshift-app-host}/
:amqoneline-url: https://console-workshop-operators.{openshift-app-host}/
:next-lab-url: https://tutorial-web-app-webapp.{openshift-app-host}/tutorial/dayinthelife-streaming.git-labs-04-CEP-and-Event-Sourcing/
:user-password: openshift
:namespace: {user-username}

ifdef::env-github[]
:next-lab-url: ../lab04/walkthrough.adoc
endif::[]

[id='cloud-native-integration']
= Lab 3 - Cloud-Native Integration with EIPs

Your IT journey on International Inc has taken you to the Moon. Is now time to apply all your knowledge around Enterprise Integration to the IoT-based farming from Fleur de Lune. You will create a couple of microservices to help them get information regarding the harvest size, quality and shipping using the cloud-native framework of Apache Camel K.

*Audience:* Enterprise Integrators, System Architects, Developers, Data Integrators

*Overview*

Enterprise Integration Patterns (EIPs) have been used in technology for 40+ years.  But are they still relevant when we attempt Cloud-Native integration?

Patterns like the Splitter, Content-Based Routing (CBR) and WireTap are still very useful when it comes to managing and integrating our data in the cloud.  Using Camel K, we are able to enhance existing streaming technologies like Kafka to help route and mediate our data through various channels.

[TIP]
====
👉 Splitter: https://www.enterpriseintegrationpatterns.com/patterns/messaging/Sequencer.html

image:images/2022-07-31T15-25-3A02-124Z.png[] 

👉 Content-Based Routing: https://www.enterpriseintegrationpatterns.com/patterns/messaging/ContentBasedRouter.html

image:images/2022-07-31T15-27-3A34-773Z.png[] 

👉 WireTap: https://www.enterpriseintegrationpatterns.com/patterns/messaging/WireTap.html

image:images/2022-07-31T15-29-3A25-664Z.png[] 
====

AMQ self-service messaging enables Developers to provision messaging when and where they need it via a web-based browser. The AMQ Online component is built on the foundation of Red Hat OpenShift, a container platform for high scalability and availability of cloud-native applications.

To provide a better customer experience, you will be closely monitoring the harvest from marshmallows farms on the moon. You will also be adding a harvest batch shipping schedule to allow Fleur de lune to be more proactive along with faster response to any emergency that may impact their business.

In this lab you will consume IoT events from an AMQ Online topic, use Enterprise Integration Patterns (EIPs) in Camel-K to split and route messages to a Datalake.  Lastly, you'll stream the messages to a Standard and Premium Shipping topic on Kafka.

image::images/1.0.0-overview.png[1.0.0-overview, role="integr8ly-img-responsive"]

*Why Red Hat?*

To solve complex cloud-native integration challenges, we need to apply EIPs to manage our data and messaging needs. Apache Camel K gives us a Kubernetes native EIP solution for tackling EIPs in the cloud.

By using Camel-K, we can enhance data streaming and apply traditional EIPs to solve cloud-native integration.

*Credentials*

Your username is: `{user-username}` +
Your password is: `{user-password}`

[type=walkthroughResource]
.Che
****
* link:{codeready-url}/[Open Eclipse Che, window="_blank"]
****

[type=walkthroughResource]
.AmqOnline
****
* link:{amqoneline-url}/[Open AMQ Online Console, window="_blank"]
****

[type=walkthroughResource,serviceName=openshift]
.Openshift
****
* link:{openshift-host}/[Open Console, window="_blank"]
****

[time=5]
[id="real-time-harvest-gathering"]
== Real time harvest information gathering

=== Eclipse Che Workspace and Setup the environment
You are setting up a topic in AMQ Online to receive event signals from Edge devices.
In this task, you'll provision a messaging topic using Red Hat's Messaging as a Service platform on OpenShift (AMQ Online).

image::images/1.1.0-diagram-1.png[1.1.0-diagram, role="integr8ly-img-responsive"]

. To provision Eclipse Che Workspace, navigate to Eclipse Che console: {codeready-url}[Eclipse Che, window="_blank", id="{context}-3"]

. Login to Che using your credentials (`{user-username}` and `{user-password}`).
+
image::images/1.1.2-login.png[1.1.2-login, role="integr8ly-img-responsive"]

. Click the **Play** button to open your workspace.  Give it a few minutes to provision and open.
+
image::images/2.1.3-open-workspace.png[2.1.3-open-workspace, role="integr8ly-img-responsive"]

. You’ll be placed in the workspace. Close the initial welcome and Readme tabs then click on the Explorer button on the left side bar.

. Click the **Workspace** button and open the `FleurDeLune/projects/harvest/simulator` folder.
+
image::images/2.1.4-che-workspace-folder.png[2.1.4-che-workspace-folder, role="integr8ly-img-responsive"]

. Select **Terminal > Open Terminal in specific container** and select the container that begins with `dil-` (followed by a 5-digit alphanumeric code).  Click it and a terminal window should open.
+
image::images/2.1.6-terminal.png[2.1.6-terminal, role="integr8ly-img-responsive"]

=== Login into the OpenShift cluster

. Finally, you will need to login into the OpenShift CLI to start interacting with the platform. For login, issue the following command:
+
[source,bash,subs="attributes+"]
----
oc login -u {user-username} -p {user-password} https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT --insecure-skip-tls-verify=true
----

. You should see something like the following (the project names may be different):
+
----
Login successful.

You have access to the following projects and can switch between them with 'oc project <projectname>':

    shared-db-earth
    shared-kafka-earth
  * user1
    user1-che
    user1-dayinthel-0605
    user1-shared-475f

Using project "user1".
Welcome! See 'oc help' to get started.
----

. Most of the work will be deploy to your own `{namespace}` project namespace, so be sure to have it as a _working_ project by executing the following command:
+
[source,bash,subs="attributes+"]
----
oc project {namespace}
----

. Once you've logged into OpenShift via the CLI, run the following commands to setup this lab. (Setup AddressSpace and Kafka Cluster)
+
[source,bash,subs="attributes+"]
----
oc -n {user-username} apply -f $CHE_PROJECTS_ROOT/FleurDeLune/projects/harvest/simulator/config/prereq.yaml
----

[TIP]
====
👉 如果之前的 module 1/2 没有做，这里就会用一个 yaml 文件部署出来，里面就是部署了2个东西，一个是 amq 的 address space，另外一个是 moon kafka topic. 
====

. Navigate to the {openshift-host}[OpenShift Developer Console, window="_blank", id="{context}-3"] and login with your OpenShift credentials (`{user-username}` and `{user-password}`).

. Click on the the Topology view (left side menu)
+
image:images/openshift-kafkas-list.png[Back To Topology]

. Wait for the Kafka cluster to start. It can take a few minutes as the operator will deploy your Kafka cluster infrastructure and related operators to manage it.
+
image:images/openshift-kafka-topology.png[Kafka Topology]

. Navigate to the {amqoneline-url}[AMQ Online Console, window="_blank", id="{context}-3"]

. Login to using your credentials (`{user-username}` and `{user-password}`).

. Make sure the AMQ AddressSpace *amq* is listed on the page
+
image::images/1.1.3-amq-console.png[1.1.3-amq-console, role="integr8ly-img-responsive"]


[type=verification]
Were you able to successfully provision the AMQ Online Address Space and Kafka Cluster?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.


=== Create a Topic
*Red Hat AMQ Online* is an OpenShift-based mechanism for delivering messaging as a managed service. With Red Hat AMQ Online, administrators can configure a cloud-native, multi-tenant messaging service where developers can provision messaging using a _web console_. Multiple development teams can provision the brokers and queues from the console, *without* requiring each team to _install, configure, deploy, maintain, or patch any software_.


. In your {amqoneline-url}[AMQ Online Console, window="_blank", id="{context}-3"].a Click *amq* listed on the page this will take you to the
+
image::images/1.1.3-amq-console.png[1.1.3-amq-console, role="integr8ly-img-responsive"]


. Click the *Create Address* button to create the topic.
+
image::images/1.1.7-create-topic.png[1.1.7-create-topic, role="integr8ly-img-responsive"]

. Enter the following details, then click *Next*:
** Name: *`mytopic`*
** Type: *topic*
** Plan: *Small Topic*
+
image::images/1.1.8-topic-details.png[1.1.8-topic-details, role="integr8ly-img-responsive"]

. Review your configuration and click on Finish
+
image::images/1.1.9-topic-details.png[1.1.9-topic-details, role="integr8ly-img-responsive"]

. Please wait a few minutes for the topic to provision.  Once the queue is provisioned, the topic name (`mytopic`) should have a green checkmark next to it.
+
image::images/1.1.10-topic-provisioned.png[1.1.10-topic-provisioned, role="integr8ly-img-responsive"]

. Now that our messaging infrastructure is deployed, we need to retrieve the messaging hostname service for our services to connect. Click on the *Endpoints* tab to check the messaging host information.
+
image::images/addressspace-endpoints.png[Address Space Endpoints, role="integr8ly-img-responsive"]

. Copy and write down the *Host* information of the `amq.messsaging.cluster`. As we will be connecting all our services from within OpenShift we can use the internal hostname.
+
image::images/addressspace-hostname.png[Address Space Host, role="integr8ly-img-responsive"]

[type=verification]
Were you able to successfully provision the topic in AMQ Online?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.


[time=15]
[id="startup-che-workspace"]
== Create the Simulator
. Since we can't really set up a real edge device for you on the moon, you will need to create a simulator that simulates the edge device that sends randomly generated harvest data.
+
image::images/1.1.0-diagram-2.png[1.1.0-diagram, role="integr8ly-img-responsive"]

. In the Che workspace open the `FleurDeLune/projects/harvest/simulator` folder.
+
image::images/2.1.4-che-workspace-folder.png[2.1.4-che-workspace-folder, role="integr8ly-img-responsive"]

. Open the `edge.properties` file.  This is the *application.properties* file where all credentials are stored.  We need to update `quarkus.qpid-jms.url` for the **AMQP** endpoint.  Copy and paste the `serviceHost` you copied earlier (into a text editor) and update the `amqp://` endpoint with the correct service hostname.
+
image::images/2.1.5-edge-properties.png[2.1.5-edge-properties, role="integr8ly-img-responsive"]

. Go to the `dil-` terminal that was opened in the previous task (followed by a 5-digit alphanumeric code).
. Run the following commands to update `edge-config` configmap.
+
[source,bash,subs="attributes+"]
----
oc project {namespace}
cd /projects/FleurDeLune/projects/harvest/simulator
oc create configmap edge-config  --from-file=edge.properties
----

. Open the `EdgeSimulator.java` file located in the same folder.  We want to create a Camel Route that fires a timer every 5 seconds, retrieves some random data, marshalls it to JSON and sends it via AMQP to your AMQ Online **mytopic**.  Copy and paste the following Camel route to your EdgeSimulator.java file:
+
[source,java,subs="attributes+"]
----
from("timer:tick?fixedRate=true&period=5000")
.choice()
    .when(simple("{{simulator.run}}"))
        .setBody(method(this, "genRandomIoTData()"))
        .marshal().json()
        .log("${body}")
        .to("amqp:topic:mytopic?subscriptionDurable=false&exchangePattern=InOnly")
    .otherwise()
        .log("Nothing send ")
;
----
+
image::images/2.1.9-edgesim.png[2.1.9-edgesim, role="integr8ly-img-responsive"]

. Try deploying and running the *EdgeSimulator* Camel-K route by executing the following command
+
[source,bash,subs="attributes+"]
----
kamel run EdgeSimulator.java
----

[TIP]
====
👉 kamel run 会创建一个 build config, 并进而创建一个 build，命令执行以后，输出是这样的，可以看到多了几个参数。link:{https://access.redhat.com/documentation/fr-fr/red_hat_integration/2021.q3/html/developing_and_managing_integrations_using_camel_k/configuring-camel-k#assembly-specifying-runtime-configuration-options}[官方文档] 在这里
----
Modeline options have been loaded from source files
Full command: kamel run EdgeSimulator.java --dependency=camel-bean --dependency=camel-jackson --configmap=edge-config 
integration "edge-simulator" created
----

👉 {{simulator.run}} 是引用了配置文件里面的值，这里面，值就是 true

👉 timer:name 是一个时间序列生成器，用法见 link:{https://camel.apache.org/components/3.18.x/timer-component.html}[文档]

👉 simple 的用法，就是简单的运行一下规则表达式，见 link:{https://camel.apache.org/components/3.18.x/languages/simple-language.html}[文档]

👉 这个程序，用EIP的脚本方式，做了一个简单的逻辑，用一个内置的时序发生器，定时产生事件，然后判断配置文件里面，是不是触发消息输出，如果配置了 true ， 就调用 java 的方法，造一个消息，然后发给amq.

👉 我们在这里，又看到了EIP脚本的能力，能简化业务逻辑代码的开发，同时也能和 java 代码片段集成。
====

. Give the deployment 2-5 minutes to run. To see the log, run the following command, and type ctrl-C/cmd-C to exit the log
+
[source,bash,subs="attributes+"]
----
kamel log edge-simulator
----

+
image::images/2.1.10-kamel-log.png[2.1.11-verify-edge-simulator, role="integr8ly-img-responsive"]

. Or you can also navigate back to the *OpenShift Developer Console* link:{openshift-host}/topology/ns/{namespace}[OpenShift Developer Console, window="_blank"] and verify the **edge-simulator** pod deployed correctly.  You can verify this by checking the Camel **timer** is firing every 5 seconds and there are no errors.

+
image::images/2.1.11-verify-edge-simulator.png[2.1.11-verify-edge-simulator, role="integr8ly-img-responsive"]

+
image::images/2.1.12-verify-edge-simulator-log.png[2.1.11-verify-edge-simulator-log, role="integr8ly-img-responsive"]

[type=verification]
Were you able to successfully deploy the Camel-K **Edge Simulator** to OpenShift?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.


[time=15]
[id="setup-order-inventory"]
== Setup Order Inventory with AMQ Streams
Now that the harvest data is now streaming into the topic, we will need to grade the marshmallows according to their sizes. First we will store all the updated grading information in a relational database. It will also stream separately to the shipping departments.

image::images/3.0.0-overview.png[3.0.0-overview, role="integr8ly-img-responsive"]

image::images/3.0.0-diagram.png[3.0.0-diagram, role="integr8ly-img-responsive"]

[TIP]
====
👉 默认的图不是太好，作者重新画了一个。这部分练习，是用一个模拟的事件发生器，产生事件，然后用一个比较复杂的EIP模型，处理这些事件/消息，然后转发到amq stream / kafka 里面去。同时，把消息备份到数据库里面做分析。
====

image:arch.drawio.svg[]

. Navigate to the {openshift-host}[OpenShift Developer Console, window="_blank", id="{context}-3"]

. Login to OpenShift Developer Console using your credentials (`{user-username}` and `{user-password}`).

. Select the *Developer* drop-down, then select *Project: {namespace}*, *+Add* and click on the `From Catalog` link.
+
image::images/3.1.3-add-from-catalog.png[3.1.3-add-from-catalog, role="integr8ly-img-responsive"]

. In the *Filter by keyword...* box, enter `Postgresql`. You may also need to un-check the *Operator Backed* checkbox on the left-hand side. Select the **PostgreSQL (Ephemeral)** template.  Click the **Instantiate Template** button.
+
image::images/3.1.5-postgres-template.png[3.1.5-postgres-template, role="integr8ly-img-responsive"]

. Update the following template details leaving the remaining default values untouched, then click **Create**:
** PostgreSQL Connection Username: *`user`*
** PostgreSQL Connection Password: *`password`*
+
image::images/3.1.6-postgres-details.png[3.1.6-postgres-details, role="integr8ly-img-responsive"]

. Wait for the pod to deploy (30 seconds - 1 minute).  Click on *Topology* then click the `postgresql` pod.
+
image::images/3.1.7-pod-details.png[3.1.7-pod-details, role="integr8ly-img-responsive"]

. Click on the *Terminal* tab and enter the following:
+
[source,bash,subs="attributes+"]
----
psql -d sampledb -U user
----

+
[source,bash,subs="attributes+"]
----
CREATE TABLE premium (
	mmid bigint NOT NULL,
	diameter integer NOT NULL,
    weight decimal NOT NULL,
	created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
----

+
[source,bash,subs="attributes+"]
----
CREATE TABLE standard (
	weight decimal NOT NULL,
	created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
----

+
[source,bash,subs="attributes+"]
----

INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
INSERT INTO premium(mmid,diameter, weight) VALUES (4567845678456, 4, 2.3);
----

. Now that we've populated the database table with records, navigate back to the *Eclipse Che* window and open the `FleurDeLune/projects/harvest/inventory` project.  Examine the `Inventory.java` file.  At this point we need to create 3 Camel routes.  A route to:
+
** Consume harvest JSON messages from AMQ Online, and using Content-based routing determine whether they are standard, premium or utility marshmallows.
** Insert premium marshmallow dimensions into the PREMIUM database table
** Insert standard marshmallow dimensions into the STANDARD database table

. Copy & paste the following Camel routes to the `Inventory.java` file:
+
[source,java,subs="attributes+"]
----
from("amqp:topic:mytopic?subscriptionDurable=false")
.split().jsonpath("$.harvest[*]")
    .choice()
        .when().jsonpath("$[?(@.diameter > 4 )]" )
            .log("Premium ${body}")
            .wireTap("direct:premiumDB")
                .newExchangeHeader("quality", constant("Premium"))
                .newExchangeHeader("diameter",jsonpath("$.diameter"))
                .newExchangeHeader("weight",jsonpath("$.weight"))
                .newExchangeHeader("mmid",jsonpath("$.mmid"))
            .end()
            .marshal().json()
            .to("kafka:{user-username}-premium?groupId=sender")
        .when().jsonpath("$[?(@.diameter > 1 )]")
            .log("Standard ${body}")
            .wireTap("direct:standardDB")
                .newExchangeHeader("quality", constant("Standard"))
                .newExchangeHeader("weight",jsonpath("$.weight"))
            .end()
            .marshal().json()
            .to("kafka:{user-username}-standard?groupId=sender")
        .otherwise()
            .log("Utility ${body}")
            .marshal().json()
            .to("kafka:{user-username}-utility?groupId=sender")
        .end()
;

from("direct:premiumDB")
    .log("inventoryDa stored ${headers.quality} diameter ${headers.diameter}")
    .setBody(simple("insert into premium (mmid,diameter,weight) VALUES (${headers.mmid},${headers.diameter},${headers.weight} )"))
    .to("jdbc:dataSource");

from("direct:standardDB")
    .log("inventoryDa stored ${headers.quality}")
    .setBody(simple("insert into standard (weight) VALUES (${headers.weight})"))
    .to("jdbc:dataSource");
----
+
image::images/3.1.8-update-inventory-java.png[3.1.8-update-inventory-java, role="integr8ly-img-responsive"]

. Return to the OpenShift Developer console, click **+Add** then click **From Catalog** link.
+
image::images/3.1.3-add-from-catalog.png[3.1.3-add-from-catalog, role="integr8ly-img-responsive"]

. In the filter box type `topic` then select **Kafka topic**.  Click **Create**.  Replace the name `my-topic` with our topic name `{user-username}-premium`, and update the cluster name to `moon`.  Click **Create**.
+
image::images/3.1.9-create-kafka-topic.png[3.1.9-create-kafka-topic, role="integr8ly-img-responsive"]

. Repeat the previous step to create `{user-username}-standard` and `{user-username}-utility` topics.

. Return to the Eclipse Che IDE and open the `kafka.properties` file located in the **FleurDeLune/projects/harvest/inventory** folder.  Update the **quarkus.qpid-jms.url** for AMQP with the same one entered in edge.properties.  Additionally, update the **kafka.brokers** URL to be `moon-kafka-bootstrap.{user-username}.svc:9092`.
+
image::images/3.1.10-update-kafka-properties.png[3.1.10-update-kafka-properties, role="integr8ly-img-responsive"]

. Return to the *dil-* terminal and execute the following commands:
+
[source,bash,subs="attributes+"]
----
oc project {namespace}
cd /projects/FleurDeLune/projects/harvest/inventory
oc create configmap sender-config  --from-file=kafka.properties
kamel run Inventory.java --name=inventory-lab3
----
+
[TIP]
====
👉 运行完以后提示：
----
Full command: kamel run Inventory.java --name=inventory-lab3 --dependency=mvn:org.postgresql:postgresql:42.2.10 --dependency=camel-jdbc --dependency=mvn:org.apache.commons:commons-dbcp2:2.7.0 --configmap=sender-config 
----
====

. After the Camel-K command has finished deploying, it should run via the terminal without errors.  You should see **Integration Created**.
+
image::images/3.1.11-camel-k-inventory.png[3.1.11-camel-k-inventory, role="integr8ly-img-responsive"]
+
[TIP]
====
👉 这里似乎有点问题，你看不到这个结果，可以忽略掉。
====

. We can verify that orders are inserted into the database tables (premium and standard), by returning to the OpenShift Developer Console, selecting postgresql and clicking the running pod.
+
image::images/3.1.7-pod-details.png[3.1.7-pod-details, role="integr8ly-img-responsive"]

. Click on the *Terminal* tab and enter the following:
+
[source,bash,subs="attributes+"]
----
psql -d sampledb -U user
----
+
[source,bash,subs="attributes+"]
----
select * from standard;
----

. If the Inventory simulator worked correctly, you should see new rows inserted into the **standard** table.

[type=verification]
Were you able to successfully view records in the **standard** database table?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.

[time=10]
[id="setup-data-lake"]
== Setup Data Lake with caching for Big Data analysis
. Due to the high inter-planet shipping costs, “Fleur de lune” is introducing a new AI system for more efficient shipping. The shipping department is now responsible for preparing all data into a in-memory data lake for the AI system.

+
image::images/4.0.0-diagram.png[3.0.0-diagram, role="integr8ly-img-responsive"]


. Navigate back to the Eclipse Che console, and open `connect-secret.yaml` and `jdg-cluster.yaml` located in `/projects/harvest/shipping`.  Take a  look and notice this will be the identity secret required to setup our Infinispan cluster.
+
image::images/4.1.1-connect-secret.png[4.1.1-connect-secret, role="integr8ly-img-responsive"]

. Lets go ahead and install both the secret and Infinispan cluster (the operator is already running for us).  Via the terminal console, execute the following commands:
+
[source,bash,subs="attributes+"]
----
cd /projects/FleurDeLune/projects/harvest/shipping
oc project {namespace}
oc create -f connect-secret.yaml
oc create -f jdg-cluster.yaml
----
+
[TIP]
====
👉 secret 解密以后，是这样的
----
credentials:
- username: developer
  password: password
- username: operator
  password: password
----

👉 之后创建了一个 infinispan / redhat datagrid 的实例，redhat datagrid 是内存数据库，对标redis，有一些异地同步/异步写的企业级功能。
====

. Navigate back to the OpenShift Developer console, select *Topology*, then click on the `example-infinispan` container.  Verify the pod has started and is running.
+
image::images/4.1.2-check-infinispan.png[4.1.2-check-infinispan, role="integr8ly-img-responsive"]

. Via the Eclipse Che IDE, open the `premiumshipping-config.yaml` file.  Update the `camel.component.kafka.brokers` to be `moon-kafka-bootstrap.{user-username}.svc:9092` and `camel.component.infinispan-configuration.hosts` URL to be `example-infinispan.{user-username}.svc:11222`.
+
image::images/4.1.4-premium-config.png[4.1.4-premium-config, role="integr8ly-img-responsive"]

. Via the terminal, execute the following command to deploy the config map:
+
[source,bash,subs="attributes+"]
----
oc apply -f premiumshipping-config.yaml
----

. Now that we have the config map deployed, let's take a look at `PremiumShipping.java`.  This class contains a Camel route which consumes messages from Kafka and populates the Infinispan cache with premium shipments. Let's insert our Camel routes into this class:
*MAKE SURE to replace the Kafka Topic to `{user-username}`*
+
[source,java,subs="attributes+"]
----
from("timer:cleanup?repeatCount=1")
.setHeader(InfinispanConstants.OPERATION).constant(InfinispanOperation.CLEAR)
.setHeader(InfinispanConstants.KEY).constant("premium")
.to("infinispan:default")
;


from("kafka:user1-premium?groupId=premium-shipping")
.streamCaching()
    .unmarshal(new JacksonDataFormat(Map.class))
    .log("Input --> ${body}")
    .setHeader("marshmallow").simple("${body}")
    .setHeader(InfinispanConstants.OPERATION).constant(InfinispanOperation.GET)
    .setHeader(InfinispanConstants.KEY).constant("premium")
    .to("infinispan:default")
    .setHeader(InfinispanConstants.OPERATION).constant(InfinispanOperation.PUT)
    .setHeader(InfinispanConstants.KEY).constant("premium")
    .setHeader(InfinispanConstants.VALUE).method(this, "assignShipment(${body}, ${header.marshmallow})")
    .log("${body}")
    .to("infinispan:default")

;
----
+
image::images/4.1.6-update-kafka-topic.png[4.1.6-update-kafka-topic, role="integr8ly-img-responsive"]
+
[TIP]
====
👉 这部分 camel / EIP 脚本逻辑是这样的。

首先，在 infinispan / data grid 内存数据库里面，就2个key，一个是premium，一个是standard。然后脚本先清空 premium key 的内存数据库对应数据，算是启动时候的一个数据归零操作。

然后，从 kafka 读取 simulator 发来的数据， simulator 的数据结果是，用 农场 / farm 做 id， 然后把收获的果子，作为一个 list 放进去。这样，从 kafka 读出来以后，先把这个数据存到 head 里面，然后从 infinispan / data grid 里面，读出来已经有的货运信息，之后，把现在已经有的货运信息，以及kafka拿来的收获信息，放给 assignShipment 去拼装出一个新的货运信息，把这个新的货运信息，存到 infinispan / datagrid 的 key 是 premium 的数据结构中。

👉 后面的 standard 类别，处理逻辑是一样的。
====

. We need to update the standard shipping config map.  Open up `standardshipping-config.yaml` file and update both the `camel.component.kafka.brokers` and `camel.component.infinispan.configuration.hosts` URLs.  You can reuse the URLs you used in the premium shipping config map.
+
image::images/4.1.7-update-standard-config.png[4.1.7-update-standard-config, role="integr8ly-img-responsive"]

. Via the terminal, execute the following command to deploy the config map:
+
[source,bash,subs="attributes+"]
----
oc apply -f standardshipping-config.yaml
----

. Now that we have the config map deployed, let's take a look at `StandardShipping.java`.  This Class contains a Camel route which consumes messages from Kafka and populates the Infinispan cache with standard shipments. Update the kafka topic name to `{user-username}-standard`.
+
image::images/4.1.8-standard-java-update.png[4.1.8-standard-java-update, role="integr8ly-img-responsive"]

. Now that we have updated all the config files and code, we need to test our Camel-K routes.  Return to the *dil-* terminal and execute the following command:
+
[source,bash,subs="attributes+"]
----
kamel run PremiumShipping.java
----
+
[TIP]
====
👉 执行完以后，输出是这样的：
----
Modeline options have been loaded from source files
Full command: kamel run PremiumShipping.java --dependency=camel-infinispan --dependency=camel-bean --dependency=camel-jackson --dependency=mvn:org.wildfly.security:wildfly-elytron:1.11.2.Final --dependency=mvn:io.netty:netty-codec:4.1.49.Final --configmap=premiumshipping-config --trait=quarkus.enabled=false 
integration "premium-shipping" created
----
====

. Ensure that the Camel-K command ran without error and connections to Infinispan and Kafka were successful.  You can verify the deployment via the OpenShift Developer topology screen.  Repeat the same for the StandardShipping flow:
+
[source,bash,subs="attributes+"]
----
kamel run StandardShipping.java
----

[type=verification]
Were you able to successfully execute and deploy both the Standard and Premium shipping Camel-K routes without error?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.

[time=5]
[id="setup-supply-console"]
== Create a Shipping Console

.  Now that we have our backend services running, we can focus on creating a Shipping Console UI.
+
image::images/5.0.0-diagram.png[5.0.0-diagram, role="integr8ly-img-responsive"]

=== Deploy RESTful Interface

.  First step is to update the `/projects/harvest/display/shippingconsole-config.yaml` config map with the correct InfiniSpan hostname.  Find  **camel.component.infinispan.configuration.hosts** and update the service to: `example-infinispan.{user-username}.svc:11222`.
+
image::images/5.1.1-config-map.png[5.1.1-config-map, role="integr8ly-img-responsive"]

. Add the config map to OpenShift using the following command (via the terminal):
+
[source,bash,subs="attributes+"]
----
cd /projects/FleurDeLune/projects/harvest/display/
oc project {namespace}
oc apply -f shippingconsole-config.yaml
----

. Now that we have the configmap updated, take a look at **ConsoleService.java**.  Notice that we use Camel RESTDsl to expose a bunch of RESTFul queries around our infinispan cache.  Let's try running this interface using the following command:
+
[source,bash,subs="attributes+"]
----
kamel run ConsoleService.java --dev
----
+
[TIP]
====
👉 命令的输出是这样的：
----
Modeline options have been loaded from source files
Full command: kamel run ConsoleService.java --dev --dependency=camel-infinispan --dependency=camel-bean --dependency=camel-jackson --dependency=camel-openapi-java --dependency=mvn:org.wildfly.security:wildfly-elytron:1.11.2.Final --dependency=mvn:io.netty:netty-codec:4.1.49.Final --configmap=shippingconsole-config --trait=quarkus.enabled=false 
integration "console-service" created
Progress: integration "console-service" in phase Initialization
Condition "IntegrationPlatformAvailable" is "True" for Integration console-service: user1/camel-ip
Integration "console-service" in phase "Initialization"
Progress: integration "console-service" in phase Building Kit
Integration "console-service" in phase "Building Kit"
Condition "IntegrationKitAvailable" is "False" for Integration console-service: creating a new integration kit
Integration Kit "kit-cbjooku2epn2nngcp45g", created by Integration "console-service", changed phase to "Build Submitted"
Build "kit-cbjooku2epn2nngcp45g", created by Integration "console-service", changed phase to "Scheduling"
Build "kit-cbjooku2epn2nngcp45g", created by Integration "console-service", changed phase to "Pending"
Build "kit-cbjooku2epn2nngcp45g", created by Integration "console-service", changed phase to "Running"
Integration Kit "kit-cbjooku2epn2nngcp45g", created by Integration "console-service", changed phase to "Build Running"
----

访问 http://console-service-user1.apps.cluster-v68vb.v68vb.sandbox904.opentlc.com/api-doc 返回是这样的：
----
{
  "openapi" : "3.0.2",
  "info" : {
    "title" : "Order API",
    "version" : "1.0"
  },
  "servers" : [ {
    "url" : "http://console-service-user1.apps.cluster-v68vb.v68vb.sandbox904.opentlc.comhttp://0.0.0.0:8080null"
  } ],
  "paths" : {
    "/premium" : {
      "get" : {
        "responses" : {
          "200" : { }
        },
        "operationId" : "verb2"
      }
    },
    "/query" : {
      "get" : {
        "responses" : {
          "200" : { }
        },
        "operationId" : "verb1"
      }
    },
    "/standard" : {
      "get" : {
        "responses" : {
          "200" : { }
        },
        "operationId" : "verb3"
      }
    }
  }
}
----

👉 rest 相关的文档，在这里。 https://camel.apache.org/manual/rest-dsl.html#_openapi_swagger_api
====

. Now that we have the Camel-K interface running, we can view the content in our Data Lake.  First, navigate here (in a new tab) to see the Standard shipments: `http://console-service-{user-username}.{openshift-app-host}/standard`. If successful, you should see output similar to the following:
+
image::images/standard-shipping-cache-web-output.png[standard-shipping-cache-web-output, role="integr8ly-img-responsive"]

. Click here (in another tab) to verify our Premium shipments cache: `http://console-service-{user-username}.{openshift-app-host}/premium`.  You should see the following output:
+
image::images/premium-cache-web-output.png[premium-cache-web-output, role="integr8ly-img-responsive"]

[type=verification]
Were you able to successfully see output from both Standard and Premium datacache?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.

[time=10]
[id="grafana"]
== Setup Grafana Dashboard

. First of all, we need to deploy the Grafana template to our namespace.  Execute the following command via the CLI *dil-* terminal:
+
[source,bash,subs="attributes+"]
----
cd /projects/FleurDeLune/projects/harvest/display

oc apply -f grafana.yaml
----
+
[source,bash,subs="attributes+"]
----
oc expose svc grafana
----

. Now that we have Grafana running, navigate back to the OpenShift Developer console and Click on grafana in the topology.  Find *Grafana* route.
+
image::images/6.1.1-grafana-route.png[6.1.1-grafana-route, role="integr8ly-img-responsive"]

. Login to Grafana using the credentials `admin/admin`.  If prompted to change your password, set it back to `admin` again.

. Now that you are logged into Grafana, we need to create a datasource. Click the `Add data source` link, then select **PostgreSQL**.
+
image::images/6.1.3-select-datasource.png[6.1.3-select-datasource, role="integr8ly-img-responsive"]

. In the DataSource entry screen, enter the following:
** Name: *`SampleDB`*
** Host: *`postgresql:5432`*
** Database: *`sampledb`*
** User: *`user`*
** Password: *`password`*
** SSL Mode: *`disable`*

. Click **Save & Test**
+
image::images/6.1.4-postgres-save.png[6.1.4-postgres-save, role="integr8ly-img-responsive"]

. Click the **+** symbol then click **Import**.  Give the dashboard a name of `FleurDeLune`.  Navigate back to Eclipse Che and copy the content from `/projects/harvest/display/FleurDeLune-Dashboard.json`.  Paste the content into the Grafana JSON window then click **Import**.
+
image::images/6.1.5-load-json.png[6.1.5-load-json, role="integr8ly-img-responsive"]

. If everything has been running correctly, you should see some Marshmallow distribution and weight metrics displayed on your graph.
+
image::images/6.1.6-graph-metrics.png[6.1.6-graph-metrics, role="integr8ly-img-responsive"]
+
[TIP]
====
👉 我们注意到，grafana是从 postgres 数据库里面直接抓取数字的，这里面的关键，是面板里面的 raw_sql 定义
----
"rawSql": "SELECT\n  $__timeGroupAlias(created_at,$__interval),\n  avg(weight) AS \"weight\"\nFROM premium\nWHERE\n  $__timeFilter(created_at)\nGROUP BY 1\nORDER BY 1",
----
====

[type=verification]
Were you able to successfully view the FleurDeLune metrics?

[type=verificationFail]
Verify that you followed each step in the procedure above. If you are still having issues, contact your administrator.

[time=5]
[id="summary"]
== Summary

In this lab you exposed inventory data via RestDSL, cached data from a Data Lake using InfiniSpan, then graphed the results using live data metrics in Grafana.

Open source connectors enable integrations with your local systems landscape. Explore InfiniSpan, Camel-K, and Grafana to connect APIs and services for event-driven application architectures (EDA). Red Hat offers supported versions of these connectors via Fuse and DataGrid.

You can now proceed to link:{next-lab-url}[Lab 4].

[time=4]
[id="further-reading"]
== Notes and Further Reading

* https://www.redhat.com/en/technologies/jboss-middleware/amq[Red Hat AMQ]
* https://developers.redhat.com/topics/event-driven/connectors/[Camel & Debezium Connectors]
